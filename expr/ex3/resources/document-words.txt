distributed
computing
is
a
field
of
computer
science
that
studies
distributed
systems
a
distributed
system
is
a
system
whose
components
are
located
on
different
networked
computers
which
communicate
and
coordinate
their
actions
by
passing
messages
to
one
another
from
any
system
the
components
interact
with
one
another
in
order
to
achieve
a
common
goal
three
significant
characteristics
of
distributed
systems
are
concurrency
of
components
lack
of
a
global
clock
and
independent
failure
of
components
examples
of
distributed
systems
vary
from
soa-based
systems
to
massively
multiplayer
online
games
to
peer-to-peer
applications
a
computer
program
that
runs
within
a
distributed
system
is
called
a
distributed
program
and
distributed
programming
is
the
process
of
writing
such
programs
there
are
many
different
types
of
implementations
for
the
message
passing
mechanism
including
pure
http
rpc-like
connectors
and
message
queues
distributed
computing
also
refers
to
the
use
of
distributed
systems
to
solve
computational
problems
in
distributed
computing
a
problem
is
divided
into
many
tasks
each
of
which
is
solved
by
one
or
more
computers
which
communicate
with
each
other
via
message
passing
introduction
the
word
distributed
in
terms
such
as
distributed
system
distributed
programming
and
distributed
algorithm
originally
referred
to
computer
networks
where
individual
computers
were
physically
distributed
within
some
geographical
area
the
terms
are
nowadays
used
in
a
much
wider
sense
even
referring
to
autonomous
processes
that
run
on
the
same
physical
computer
and
interact
with
each
other
by
message
passing
while
there
is
no
single
definition
of
a
distributed
system
the
following
defining
properties
are
commonly
used
as
there
are
several
autonomous
computational
entities
computers
or
nodes
each
of
which
has
its
own
local
memory
the
entities
communicate
with
each
other
by
message
passing
a
distributed
system
may
have
a
common
goal
such
as
solving
a
large
computational
problem
the
user
then
perceives
the
collection
of
autonomous
processors
as
a
unit
alternatively
each
computer
may
have
its
own
user
with
individual
needs
and
the
purpose
of
the
distributed
system
is
to
coordinate
the
use
of
shared
resources
or
provide
communication
services
to
the
users
other
typical
properties
of
distributed
systems
include
the
following
the
system
has
to
tolerate
failures
in
individual
computers
the
structure
of
the
system
network
topology
network
latency
number
of
computers
is
not
known
in
advance
the
system
may
consist
of
different
kinds
of
computers
and
network
links
and
the
system
may
change
during
the
execution
of
a
distributed
program
each
computer
has
only
a
limited
incomplete
view
of
the
system
each
computer
may
know
only
one
part
of
the
input
parallel
and
distributed
computing
distributed
systems
are
groups
of
networked
computers
which
share
a
common
goal
for
their
work
the
terms
concurrent
computing
parallel
computing
and
distributed
computing
have
much
overlap
and
no
clear
distinction
exists
between
them
the
same
system
may
be
characterized
both
as
parallel
and
distributed
the
processors
in
a
typical
distributed
system
run
concurrently
in
parallel
parallel
computing
may
be
seen
as
a
particular
tightly
coupled
form
of
distributed
computing
and
distributed
computing
may
be
seen
as
a
loosely
coupled
form
of
parallel
computing
nevertheless
it
is
possible
to
roughly
classify
concurrent
systems
as
parallel
or
distributed
using
the
following
criteria
in
parallel
computing
all
processors
may
have
access
to
a
shared
memory
to
exchange
information
between
processors
in
distributed
computing
each
processor
has
its
own
private
memory
distributed
memory
information
is
exchanged
by
passing
messages
between
the
processors
the
figure
on
the
right
illustrates
the
difference
between
distributed
and
parallel
systems
figure
a
is
a
schematic
view
of
a
typical
distributed
system
the
system
is
represented
as
a
network
topology
in
which
each
node
is
a
computer
and
each
line
connecting
the
nodes
is
a
communication
link
figure
b
shows
the
same
distributed
system
in
more
detail
each
computer
has
its
own
local
memory
and
information
can
be
exchanged
only
by
passing
messages
from
one
node
to
another
by
using
the
available
communication
links
figure
c
shows
a
parallel
system
in
which
each
processor
has
a
direct
access
to
a
shared
memory
the
situation
is
further
complicated
by
the
traditional
uses
of
the
terms
parallel
and
distributed
algorithm
that
do
not
quite
match
the
above
definitions
of
parallel
and
distributed
systems
see
below
for
more
detailed
discussion
nevertheless
as
a
rule
of
thumb
high-performance
parallel
computation
in
a
shared-memory
multiprocessor
uses
parallel
algorithms
while
the
coordination
of
a
large-scale
distributed
system
uses
distributed
algorithms
history
the
use
of
concurrent
processes
which
communicate
through
message-passing
has
its
roots
in
operating
system
architectures
studied
in
the
1960s
the
first
widespread
distributed
systems
were
local-area
networks
such
as
ethernet
which
was
invented
in
the
1970s
arpanet
one
of
the
predecessors
of
the
internet
was
introduced
in
the
late
1960s
and
arpanet
e-mail
was
invented
in
the
early
1970s
e-mail
became
the
most
successful
application
of
arpanet
and
it
is
probably
the
earliest
example
of
a
large-scale
distributed
application
in
addition
to
arpanet
and
its
successor
the
global
internet
other
early
worldwide
computer
networks
included
usenet
and
fidonet
from
the
1980s
both
of
which
were
used
to
support
distributed
discussion
systems
the
study
of
distributed
computing
became
its
own
branch
of
computer
science
in
the
late
1970s
and
early
1980s
the
first
conference
in
the
field
symposium
on
principles
of
distributed
computing
podc
dates
back
to
1982
and
its
counterpart
international
symposium
on
distributed
computing
disc
was
first
held
in
ottawa
in
1985
as
the
international
workshop
on
distributed
algorithms
on
graphs
architectures
various
hardware
and
software
architectures
are
used
for
distributed
computing
at
a
lower
level
it
is
necessary
to
interconnect
multiple
cpus
with
some
sort
of
network
regardless
of
whether
that
network
is
printed
onto
a
circuit
board
or
made
up
of
loosely
coupled
devices
and
cables
at
a
higher
level
it
is
necessary
to
interconnect
processes
running
on
those
cpus
with
some
sort
of
communication
system
distributed
programming
typically
falls
into
one
of
several
basic
architectures
client–server
three-tier
n-tier
or
peer-to-peer
or
categories
loose
coupling
or
tight
coupling
client–server
architectures
where
smart
clients
contact
the
server
for
data
then
format
and
display
it
to
the
users
input
at
the
client
is
committed
back
to
the
server
when
it
represents
a
permanent
change
three-tier
architectures
that
move
the
client
intelligence
to
a
middle
tier
so
that
stateless
clients
can
be
used
this
simplifies
application
deployment
most
web
applications
are
three-tier
n-tier
architectures
that
refer
typically
to
web
applications
which
further
forward
their
requests
to
other
enterprise
services
this
type
of
application
is
the
one
most
responsible
for
the
success
of
application
servers
peer-to-peer
architectures
where
there
are
no
special
machines
that
provide
a
service
or
manage
the
network
resources
instead
all
responsibilities
are
uniformly
divided
among
all
machines
known
as
peers
peers
can
serve
both
as
clients
and
as
servers
examples
of
this
architecture
include
bittorrent
and
the
bitcoin
network
another
basic
aspect
of
distributed
computing
architecture
is
the
method
of
communicating
and
coordinating
work
among
concurrent
processes
through
various
message
passing
protocols
processes
may
communicate
directly
with
one
another
typically
in
a
master
slave
relationship
alternatively
a
database-centric
architecture
can
enable
distributed
computing
to
be
done
without
any
form
of
direct
inter-process
communication
by
utilizing
a
shared
database
database-centric
architecture
in
particular
provides
relational
processing
analytics
in
a
schematic
architecture
allowing
for
live
environment
relay
this
enables
distributed
computing
functions
both
within
and
beyond
the
parameters
of
a
networked
database
distributed
computing
is
a
field
of
computer
science
that
studies
distributed
systems
a
distributed
system
is
a
system
whose
components
are
located
on
different
networked
computers
which
communicate
and
coordinate
their
actions
by
passing
messages
to
one
another
from
any
system
the
components
interact
with
one
another
in
order
to
achieve
a
common
goal
three
significant
characteristics
of
distributed
systems
are
concurrency
of
components
lack
of
a
global
clock
and
independent
failure
of
components
examples
of
distributed
systems
vary
from
soa-based
systems
to
massively
multiplayer
online
games
to
peer-to-peer
applications
a
computer
program
that
runs
within
a
distributed
system
is
called
a
distributed
program
and
distributed
programming
is
the
process
of
writing
such
programs
there
are
many
different
types
of
implementations
for
the
message
passing
mechanism
including
pure
http
rpc-like
connectors
and
message
queues
distributed
computing
also
refers
to
the
use
of
distributed
systems
to
solve
computational
problems
in
distributed
computing
a
problem
is
divided
into
many
tasks
each
of
which
is
solved
by
one
or
more
computers
which
communicate
with
each
other
via
message
passing
introduction
the
word
distributed
in
terms
such
as
distributed
system
distributed
programming
and
distributed
algorithm
originally
referred
to
computer
networks
where
individual
computers
were
physically
distributed
within
some
geographical
area
the
terms
are
nowadays
used
in
a
much
wider
sense
even
referring
to
autonomous
processes
that
run
on
the
same
physical
computer
and
interact
with
each
other
by
message
passing
while
there
is
no
single
definition
of
a
distributed
system
the
following
defining
properties
are
commonly
used
as
there
are
several
autonomous
computational
entities
computers
or
nodes
each
of
which
has
its
own
local
memory
the
entities
communicate
with
each
other
by
message
passing
a
distributed
system
may
have
a
common
goal
such
as
solving
a
large
computational
problem
the
user
then
perceives
the
collection
of
autonomous
processors
as
a
unit
alternatively
each
computer
may
have
its
own
user
with
individual
needs
and
the
purpose
of
the
distributed
system
is
to
coordinate
the
use
of
shared
resources
or
provide
communication
services
to
the
users
other
typical
properties
of
distributed
systems
include
the
following
the
system
has
to
tolerate
failures
in
individual
computers
the
structure
of
the
system
network
topology
network
latency
number
of
computers
is
not
known
in
advance
the
system
may
consist
of
different
kinds
of
computers
and
network
links
and
the
system
may
change
during
the
execution
of
a
distributed
program
each
computer
has
only
a
limited
incomplete
view
of
the
system
each
computer
may
know
only
one
part
of
the
input
parallel
and
distributed
computing
distributed
systems
are
groups
of
networked
computers
which
share
a
common
goal
for
their
work
the
terms
concurrent
computing
parallel
computing
and
distributed
computing
have
much
overlap
and
no
clear
distinction
exists
between
them
the
same
system
may
be
characterized
both
as
parallel
and
distributed
the
processors
in
a
typical
distributed
system
run
concurrently
in
parallel
parallel
computing
may
be
seen
as
a
particular
tightly
coupled
form
of
distributed
computing
and
distributed
computing
may
be
seen
as
a
loosely
coupled
form
of
parallel
computing
nevertheless
it
is
possible
to
roughly
classify
concurrent
systems
as
parallel
or
distributed
using
the
following
criteria
in
parallel
computing
all
processors
may
have
access
to
a
shared
memory
to
exchange
information
between
processors
in
distributed
computing
each
processor
has
its
own
private
memory
distributed
memory
information
is
exchanged
by
passing
messages
between
the
processors
the
figure
on
the
right
illustrates
the
difference
between
distributed
and
parallel
systems
figure
a
is
a
schematic
view
of
a
typical
distributed
system
the
system
is
represented
as
a
network
topology
in
which
each
node
is
a
computer
and
each
line
connecting
the
nodes
is
a
communication
link
figure
b
shows
the
same
distributed
system
in
more
detail
each
computer
has
its
own
local
memory
and
information
can
be
exchanged
only
by
passing
messages
from
one
node
to
another
by
using
the
available
communication
links
figure
c
shows
a
parallel
system
in
which
each
processor
has
a
direct
access
to
a
shared
memory
the
situation
is
further
complicated
by
the
traditional
uses
of
the
terms
parallel
and
distributed
algorithm
that
do
not
quite
match
the
above
definitions
of
parallel
and
distributed
systems
see
below
for
more
detailed
discussion
nevertheless
as
a
rule
of
thumb
high-performance
parallel
computation
in
a
shared-memory
multiprocessor
uses
parallel
algorithms
while
the
coordination
of
a
large-scale
distributed
system
uses
distributed
algorithms
history
the
use
of
concurrent
processes
which
communicate
through
message-passing
has
its
roots
in
operating
system
architectures
studied
in
the
1960s
the
first
widespread
distributed
systems
were
local-area
networks
such
as
ethernet
which
was
invented
in
the
1970s
arpanet
one
of
the
predecessors
of
the
internet
was
introduced
in
the
late
1960s
and
arpanet
e-mail
was
invented
in
the
early
1970s
e-mail
became
the
most
successful
application
of
arpanet
and
it
is
probably
the
earliest
example
of
a
large-scale
distributed
application
in
addition
to
arpanet
and
its
successor
the
global
internet
other
early
worldwide
computer
networks
included
usenet
and
fidonet
from
the
1980s
both
of
which
were
used
to
support
distributed
discussion
systems
the
study
of
distributed
computing
became
its
own
branch
of
computer
science
in
the
late
1970s
and
early
1980s
the
first
conference
in
the
field
symposium
on
principles
of
distributed
computing
podc
dates
back
to
1982
and
its
counterpart
international
symposium
on
distributed
computing
disc
was
first
held
in
ottawa
in
1985
as
the
international
workshop
on
distributed
algorithms
on
graphs
architectures
various
hardware
and
software
architectures
are
used
for
distributed
computing
at
a
lower
level
it
is
necessary
to
interconnect
multiple
cpus
with
some
sort
of
network
regardless
of
whether
that
network
is
printed
onto
a
circuit
board
or
made
up
of
loosely
coupled
devices
and
cables
at
a
higher
level
it
is
necessary
to
interconnect
processes
running
on
those
cpus
with
some
sort
of
communication
system
distributed
programming
typically
falls
into
one
of
several
basic
architectures
client–server
three-tier
n-tier
or
peer-to-peer
or
categories
loose
coupling
or
tight
coupling
client–server
architectures
where
smart
clients
contact
the
server
for
data
then
format
and
display
it
to
the
users
input
at
the
client
is
committed
back
to
the
server
when
it
represents
a
permanent
change
three-tier
architectures
that
move
the
client
intelligence
to
a
middle
tier
so
that
stateless
clients
can
be
used
this
simplifies
application
deployment
most
web
applications
are
three-tier
n-tier
architectures
that
refer
typically
to
web
applications
which
further
forward
their
requests
to
other
enterprise
services
this
type
of
application
is
the
one
most
responsible
for
the
success
of
application
servers
peer-to-peer
architectures
where
there
are
no
special
machines
that
provide
a
service
or
manage
the
network
resources
instead
all
responsibilities
are
uniformly
divided
among
all
machines
known
as
peers
peers
can
serve
both
as
clients
and
as
servers
examples
of
this
architecture
include
bittorrent
and
the
bitcoin
network
another
basic
aspect
of
distributed
computing
architecture
is
the
method
of
communicating
and
coordinating
work
among
concurrent
processes
through
various
message
passing
protocols
processes
may
communicate
directly
with
one
another
typically
in
a
master
slave
relationship
alternatively
a
database-centric
architecture
can
enable
distributed
computing
to
be
done
without
any
form
of
direct
inter-process
communication
by
utilizing
a
shared
database
database-centric
architecture
in
particular
provides
relational
processing
analytics
in
a
schematic
architecture
allowing
for
live
environment
relay
this
enables
distributed
computing
functions
both
within
and
beyond
the
parameters
of
a
networked
database
